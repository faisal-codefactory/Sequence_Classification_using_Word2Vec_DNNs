{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('py36tfkeras': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f4de7bc2857907268b26b780683c7c9dfeeafee76be87614ae3bfd35a160308c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from time import time\n",
    "import numpy as np\n",
    "import os\n",
    "from SznToolBox import SznToolBox\n",
    "from Bio import SeqIO\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posname = \"usr60-5mc-pos.fasta\"\n",
    "negname = \"usr60-5mc-neg.fasta\"\n",
    "pospath= posname\n",
    "negpath= negname\n",
    "toolbox = SznToolBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fasta_Char_Iter(object):\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "\n",
    "    def __iter__(self):\n",
    "        fasta_handle = open(self.fname,'r')\n",
    "        for record in SeqIO.parse(fasta_handle,\"fasta\"):\n",
    "            for mchar in str(record.seq):\n",
    "                yield mchar\n",
    "            #print (mchar)\n",
    "            #yield line.split()    \n",
    "        fasta_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "738\n738\n"
     ]
    }
   ],
   "source": [
    "tmpfile= \"temp.fasta\"\n",
    "tmp_iter = Fasta_Char_Iter(tmpfile)\n",
    "tmp = list(tmp_iter)\n",
    "print(len(tmp))\n",
    "tmp2 = [*tmp_iter]\n",
    "print(len(tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "pos_5mc_iter = Fasta_Char_Iter(pospath)\n",
    "neg_5mc_iter = Fasta_Char_Iter(negpath)\n",
    "workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training took 16.519062995910645 sec\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "NuAcid_embed_model = gensim.models.Word2Vec(pos_5mc_iter, size=20, window=20, workers=workers, sg=1)\n",
    "toc=time()\n",
    "print(\"Training took {} sec\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training took 27.33872151374817 sec\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "NuAcid_embed_model.train(neg_5mc_iter,total_examples= 200000, epochs=1)\n",
    "toc=time()\n",
    "print(\"Training took {} sec\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfname = \"gensim_5mc_NuAcid_embeddings.gen\"\n",
    "NuAcid_embed_model.save(wfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'G': <gensim.models.keyedvectors.Vocab object at 0x0000016E397BF780>, 'A': <gensim.models.keyedvectors.Vocab object at 0x0000016E39D06278>, 'C': <gensim.models.keyedvectors.Vocab object at 0x0000016E39D063C8>, 'T': <gensim.models.keyedvectors.Vocab object at 0x0000016E39BB6908>}\n4\n(4, 20)\n"
     ]
    }
   ],
   "source": [
    "print(NuAcid_embed_model.wv.vocab)\n",
    "print(len(NuAcid_embed_model.wv.vocab))\n",
    "print(NuAcid_embed_model.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n[0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 1, 2, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 1, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "seq= 'GGAGCGGGCCCGGGCGGCGGCGGCAGCAGCGGCGACGGCTG'\n",
    "print(NuAcid_embed_model.wv.vocab['C'].index)\n",
    "tmp = [NuAcid_embed_model.wv.vocab[a].index for a in seq]\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sample_to_gensim_embedding_index(sample):\n",
    "    #sample = sample.decode('UTF-8')\n",
    "    n_arr = [NuAcid_embed_model.wv.vocab[a].index for a in sample]\n",
    "    return np.array(n_arr)"
   ]
  },
  {
   "source": [
    "##Loading Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47976\n",
      "47977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "my_n =1\n",
    "pos_sample_list = list()\n",
    "neg_sample_list = list()\n",
    "with open (pospath, \"r\") as pos_handle:\n",
    "    for record in SeqIO.parse(pos_handle,\"fasta\"):\n",
    "        pos_sample_list.append(str(record.seq))\n",
    "print(len(pos_sample_list))\n",
    "\n",
    "with open (negpath, \"r\") as neg_handle:\n",
    "    for i,record in enumerate(SeqIO.parse(neg_handle,\"fasta\")):\n",
    "        neg_sample_list.append(str(record.seq))\n",
    "        if (i==len(pos_sample_list)*my_n): # to read negative records equal to n time \n",
    "            #positive records\n",
    "        #if (i==150000):\n",
    "            break\n",
    "print(len(neg_sample_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(47976, 41)\n(47977, 41)\n[1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 2 1 1 1 1 0 0 1 1 2 1 0 1 1 1 1 3 1 0 1 0\n 1 3 3 1]\n"
     ]
    }
   ],
   "source": [
    "pos_sample_list = np.array([encode_sample_to_gensim_embedding_index(a) for a in pos_sample_list])\n",
    "neg_sample_list = np.array([encode_sample_to_gensim_embedding_index(a) for a in neg_sample_list])\n",
    "print(pos_sample_list.shape)\n",
    "print(neg_sample_list.shape)\n",
    "print(pos_sample_list[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(95953, 41)\n[0 0 2 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 1 2 0 1 2 0 1 0 0 1 0 2 1 0\n 0 1 3 0]\n[1 2 0 0 0 0 1 0 1 2 0 0 0 2 0 0 0 2 0 0 1 1 3 0 0 0 1 1 3 1 1 0 3 3 3 1 1\n 1 0 3 3]\n(array([0, 1]), array([47977, 47976], dtype=int64))\nAfter Shuffle \n X Shape: (95953, 41), Y shape: (95953,)\nX_train:(67167, 41), y_train:(67167,), X_test:(28786, 41),y_test: (28786,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "pos_list_size = len(pos_sample_list)\n",
    "neg_list_size = len(neg_sample_list)\n",
    "### Preparing labels for positive and negative data\n",
    "tmp1 = np.ones((pos_list_size,), dtype= int)\n",
    "tmp2 = np.zeros((neg_list_size,), dtype= int)\n",
    "Y = np.hstack((tmp1,tmp2))\n",
    "### Preparing and shuffling samples\n",
    "X = np.concatenate((pos_sample_list, neg_sample_list), axis = 0)\n",
    "print(X.shape)\n",
    "print(X[0])\n",
    "X,Y = shuffle(X,Y, random_state=25)\n",
    "X,Y = shuffle(X,Y, random_state=31)\n",
    "print(X[0])\n",
    "print(np.unique(Y, return_counts=True))\n",
    "print(\"After Shuffle \\n X Shape: {}, Y shape: {}\".format(X.shape, Y.shape))\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,Y, test_size=0.3,random_state=17)\n",
    "print(\"X_train:{}, y_train:{}, X_test:{},y_test: {}\".format(X_train.shape,\\\n",
    "                                                y_train.shape,X_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\n['G', 'A', 'C', 'T']\n"
     ]
    }
   ],
   "source": [
    "#char_index = \n",
    "mychars=list(NuAcid_embed_model.wv.vocab)\n",
    "print(type(NuAcid_embed_model.wv.vocab))\n",
    "print(mychars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4, 20)\n"
     ]
    }
   ],
   "source": [
    "num_chars = 4\n",
    "EMBED_DIM = 20\n",
    "embedding_matrix = np.zeros((num_chars,EMBED_DIM))\n",
    "for i,mchar in enumerate(NuAcid_embed_model.wv.vocab.keys()):\n",
    "    #print(i,mchar)\n",
    "    vector = NuAcid_embed_model.wv.get_vector(mchar)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i] = vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Bidirectional, Dropout, Embedding, Dense, LSTM, SimpleRNN, GRU,Conv1D, MaxPool1D\n",
    "from keras.layers import CuDNNLSTM,CuDNNGRU,GlobalAveragePooling1D\n",
    "#from keras.layers.advanced_activations import LeakyReLU, PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\nW0314 22:29:16.694295 10360 deprecation_wrapper.py:119] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "NA_embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "                      weights=[embedding_matrix], trainable= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 20)          80        \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, None, 10)          1410      \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, None, 20)          1020      \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, None, 20)          0         \n_________________________________________________________________\ndropout_6 (Dropout)          (None, None, 20)          0         \n_________________________________________________________________\nconv1d_3 (Conv1D)            (None, None, 30)          3030      \n_________________________________________________________________\nconv1d_4 (Conv1D)            (None, None, 48)          4368      \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, None, 48)          0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, None, 48)          0         \n_________________________________________________________________\nconv1d_5 (Conv1D)            (None, None, 64)          9280      \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 64)                0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 8)                 520       \n_________________________________________________________________\ndense_12 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 19,717\nTrainable params: 19,637\nNon-trainable params: 80\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "#model_cnn.add(Embedding(vocabulary_size, embedding_dim))\n",
    "model_cnn.add(NA_embeddings)\n",
    "model_cnn.add(Conv1D(10,7,activation=\"relu\"))#, input_shape = (None,input_size)))\n",
    "model_cnn.add(Conv1D(20,5,activation=\"relu\"))#, input_shape = (None,input_size)))\n",
    "#model_cnn.add(Conv1D(7,3,activation=\"relu\", input_shape = (None,input_size)))\n",
    "model_cnn.add(MaxPool1D(2))\n",
    "model_cnn.add(Dropout(0.50))\n",
    "model_cnn.add(Conv1D(30,5,activation=\"relu\"))\n",
    "model_cnn.add(Conv1D(48,3,activation=\"relu\"))\n",
    "model_cnn.add(MaxPool1D(2))\n",
    "model_cnn.add(Dropout(0.50))\n",
    "model_cnn.add(Conv1D(64,3,activation=\"relu\"))\n",
    "#model_cnn.add(Conv1D(24,3,activation=\"relu\"))\n",
    "#model_cnn.add(Flatten())\n",
    "model_cnn.add(GlobalAveragePooling1D())\n",
    "#model_cnn.add(Dropout(0.50))\n",
    "model_cnn.add(Dense(8, activation=\"relu\"))\n",
    "model_cnn.add(Dense(1, activation=\"sigmoid\"))\n",
    "model_cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model_cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping, TerminateOnNaN, ReduceLROnPlateau\n",
    "#epochs=100\n",
    "checkpointer = ModelCheckpoint(filepath=\"5mcCNN2.h5\", verbose=0, \\\n",
    "                               save_weights_only=False, save_best_only=True, period =5)\n",
    "earlystop = EarlyStopping(patience=5)\n",
    "TonNaN = TerminateOnNaN()\n",
    "rLR= ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 53733 samples, validate on 13434 samples\n",
      "Epoch 1/30\n",
      " - 4s - loss: 0.1849 - acc: 0.9241 - val_loss: 0.1703 - val_acc: 0.9290\n",
      "Epoch 2/30\n",
      " - 4s - loss: 0.1859 - acc: 0.9242 - val_loss: 0.1724 - val_acc: 0.9298\n",
      "Epoch 3/30\n",
      " - 4s - loss: 0.1833 - acc: 0.9253 - val_loss: 0.1727 - val_acc: 0.9276\n",
      "Epoch 4/30\n",
      " - 4s - loss: 0.1846 - acc: 0.9235 - val_loss: 0.1681 - val_acc: 0.9299\n",
      "Epoch 5/30\n",
      " - 4s - loss: 0.1834 - acc: 0.9248 - val_loss: 0.1674 - val_acc: 0.9307\n",
      "Epoch 6/30\n",
      " - 4s - loss: 0.1845 - acc: 0.9249 - val_loss: 0.1670 - val_acc: 0.9307\n",
      "Epoch 7/30\n",
      " - 4s - loss: 0.1826 - acc: 0.9253 - val_loss: 0.1730 - val_acc: 0.9264\n",
      "Epoch 8/30\n",
      " - 4s - loss: 0.1832 - acc: 0.9253 - val_loss: 0.1699 - val_acc: 0.9289\n",
      "Epoch 9/30\n",
      " - 4s - loss: 0.1829 - acc: 0.9253 - val_loss: 0.1686 - val_acc: 0.9293\n",
      "Epoch 10/30\n",
      " - 4s - loss: 0.1824 - acc: 0.9265 - val_loss: 0.1693 - val_acc: 0.9303\n",
      "Epoch 11/30\n",
      " - 4s - loss: 0.1810 - acc: 0.9269 - val_loss: 0.1673 - val_acc: 0.9313\n",
      "CNN Model training took 44.320422410964966 Secs\n"
     ]
    }
   ],
   "source": [
    "tic = time()\n",
    "hist = model_cnn.fit(X_train, y_train,verbose=2,\n",
    "          batch_size= 128,\n",
    "          callbacks=[checkpointer,earlystop,TonNaN, rLR],\n",
    "          epochs=30, validation_split=0.2)\n",
    "          #validation_data= (X_test, y_test))\n",
    "toc = time()\n",
    "print(\"CNN Model training took {} Secs\".format ((toc-tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"CNN\"]= model_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "ypred_dict =dict()\n",
    "ypredprob_dict = dict()\n",
    "model_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[12817  1592]\n [  393 13984]]\n              precision    recall  f1-score   support\n\n           0       0.97      0.89      0.93     14409\n           1       0.90      0.97      0.93     14377\n\n    accuracy                           0.93     28786\n   macro avg       0.93      0.93      0.93     28786\nweighted avg       0.93      0.93      0.93     28786\n\n0.9310428680608629\n0.8651047610017965\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_cnn.predict(X_test)\n",
    "y_pred = np.where (y_pred < 0.53, 0, 1)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.matthews_corrcoef(y_test,y_pred))\n",
    "ypred_dict[\"CNN\"] = y_pred\n",
    "ypredprob_dict[\"CNN\"] = model_cnn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 20)          80        \n_________________________________________________________________\ncu_dnngru_1 (CuDNNGRU)       (None, None, 30)          4680      \n_________________________________________________________________\ncu_dnngru_2 (CuDNNGRU)       (None, 20)                3120      \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 20)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                210       \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 8,101\nTrainable params: 8,021\nNon-trainable params: 80\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model_gru = Sequential()\n",
    "model_gru.add(NA_embeddings)\n",
    "#model.add(Dropout(dropout_rate))\n",
    "#model_bd_gru.add(Bidirectional(GRU(rnn_hidden_size)))\n",
    "model_gru.add(CuDNNGRU(30, return_sequences=True))\n",
    "model_gru.add(CuDNNGRU(20, return_sequences=False))\n",
    "model_gru.add(Dropout(0.5))\n",
    "model_gru.add(Dense(10, activation='relu'))\n",
    "#model_gru.add(Dropout(0.1))\n",
    "model_gru.add(Dense(1, activation='sigmoid'))\n",
    "model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 53733 samples, validate on 13434 samples\n",
      "Epoch 1/60\n",
      " - 49s - loss: 0.5280 - acc: 0.7093 - val_loss: 0.2559 - val_acc: 0.8956\n",
      "Epoch 2/60\n",
      " - 48s - loss: 0.2540 - acc: 0.8991 - val_loss: 0.2552 - val_acc: 0.9078\n",
      "Epoch 3/60\n",
      " - 48s - loss: 0.2381 - acc: 0.9041 - val_loss: 0.2207 - val_acc: 0.9093\n",
      "Epoch 4/60\n",
      " - 47s - loss: 0.2326 - acc: 0.9034 - val_loss: 0.2089 - val_acc: 0.9120\n",
      "Epoch 5/60\n",
      " - 48s - loss: 0.2349 - acc: 0.9008 - val_loss: 0.3398 - val_acc: 0.8365\n",
      "Epoch 6/60\n",
      " - 49s - loss: 0.2304 - acc: 0.9029 - val_loss: 0.1981 - val_acc: 0.9170\n",
      "Epoch 7/60\n",
      " - 48s - loss: 0.2155 - acc: 0.9093 - val_loss: 0.2260 - val_acc: 0.9114\n",
      "Epoch 8/60\n",
      " - 47s - loss: 0.2139 - acc: 0.9091 - val_loss: 0.1917 - val_acc: 0.9183\n",
      "Epoch 9/60\n",
      " - 47s - loss: 0.2101 - acc: 0.9108 - val_loss: 0.1925 - val_acc: 0.9173\n",
      "Epoch 10/60\n",
      " - 47s - loss: 0.2039 - acc: 0.9140 - val_loss: 0.1828 - val_acc: 0.9238\n",
      "Epoch 11/60\n",
      " - 50s - loss: 0.2007 - acc: 0.9143 - val_loss: 0.1800 - val_acc: 0.9241\n",
      "Epoch 12/60\n",
      " - 48s - loss: 0.1956 - acc: 0.9168 - val_loss: 0.1766 - val_acc: 0.9262\n",
      "Epoch 13/60\n",
      " - 48s - loss: 0.1935 - acc: 0.9190 - val_loss: 0.1756 - val_acc: 0.9289\n",
      "Epoch 14/60\n",
      " - 48s - loss: 0.1953 - acc: 0.9164 - val_loss: 0.1750 - val_acc: 0.9261\n",
      "Epoch 15/60\n",
      " - 48s - loss: 0.1877 - acc: 0.9216 - val_loss: 0.1724 - val_acc: 0.9291\n",
      "Epoch 16/60\n",
      " - 48s - loss: 0.1873 - acc: 0.9231 - val_loss: 0.1802 - val_acc: 0.9248\n",
      "Epoch 17/60\n",
      " - 48s - loss: 0.1858 - acc: 0.9231 - val_loss: 0.1702 - val_acc: 0.9284\n",
      "Epoch 18/60\n",
      " - 48s - loss: 0.1818 - acc: 0.9247 - val_loss: 0.1679 - val_acc: 0.9311\n",
      "Epoch 19/60\n",
      " - 48s - loss: 0.1810 - acc: 0.9264 - val_loss: 0.1812 - val_acc: 0.9271\n",
      "Epoch 20/60\n",
      " - 48s - loss: 0.1820 - acc: 0.9255 - val_loss: 0.1655 - val_acc: 0.9319\n",
      "Epoch 21/60\n",
      " - 47s - loss: 0.1781 - acc: 0.9280 - val_loss: 0.1657 - val_acc: 0.9323\n",
      "Epoch 22/60\n",
      " - 47s - loss: 0.1778 - acc: 0.9283 - val_loss: 0.1629 - val_acc: 0.9334\n",
      "Epoch 23/60\n",
      " - 47s - loss: 0.1766 - acc: 0.9282 - val_loss: 0.1732 - val_acc: 0.9331\n",
      "Epoch 24/60\n",
      " - 47s - loss: 0.1779 - acc: 0.9279 - val_loss: 0.1633 - val_acc: 0.9336\n",
      "Epoch 25/60\n",
      " - 48s - loss: 0.1745 - acc: 0.9299 - val_loss: 0.1647 - val_acc: 0.9333\n",
      "Epoch 26/60\n",
      " - 49s - loss: 0.1760 - acc: 0.9276 - val_loss: 0.1622 - val_acc: 0.9340\n",
      "Epoch 27/60\n",
      " - 47s - loss: 0.1762 - acc: 0.9291 - val_loss: 0.1633 - val_acc: 0.9338\n",
      "Epoch 28/60\n",
      " - 47s - loss: 0.1709 - acc: 0.9312 - val_loss: 0.1663 - val_acc: 0.9338\n",
      "Epoch 29/60\n",
      " - 49s - loss: 0.1722 - acc: 0.9315 - val_loss: 0.1608 - val_acc: 0.9361\n",
      "Epoch 30/60\n",
      " - 48s - loss: 0.1735 - acc: 0.9295 - val_loss: 0.1681 - val_acc: 0.9349\n",
      "Epoch 31/60\n",
      " - 49s - loss: 0.1717 - acc: 0.9309 - val_loss: 0.1663 - val_acc: 0.9318\n",
      "Epoch 32/60\n",
      " - 47s - loss: 0.1743 - acc: 0.9297 - val_loss: 0.1613 - val_acc: 0.9354\n",
      "Epoch 33/60\n",
      " - 47s - loss: 0.1680 - acc: 0.9334 - val_loss: 0.1647 - val_acc: 0.9317\n",
      "Epoch 34/60\n",
      " - 47s - loss: 0.1692 - acc: 0.9321 - val_loss: 0.1637 - val_acc: 0.9339\n",
      "Model training took 1627.9324629306793 Secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "chkptr_gru = ModelCheckpoint(filepath=\"5mcGRU.h5\", verbose=0, \\\n",
    "                               save_weights_only=False, save_best_only=True, period =5)\n",
    "tic = time()\n",
    "hist_srnn= model_gru.fit(X_train, y_train, verbose=2,\n",
    "          batch_size=12,\n",
    "          callbacks=[chkptr_gru,earlystop,TonNaN, rLR],\n",
    "          epochs=60,validation_split= 0.2)\n",
    "#validation_data=(X_test, y_test))\n",
    "toc = time()\n",
    "print(\"Model training took {} Secs\".format ((toc-tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"CNN\"] = model_cnn\n",
    "model_dict[\"GRU\"] = model_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_dict_path = Path(\"5mc-2models-dict21Mar05.jbl\")\n",
    "model_dict_path.touch(exist_ok=True)\n",
    "\n",
    "with open(model_dict_path, \"wb\") as file_handle:\n",
    "    joblib.dump(model_dict, file_handle)"
   ]
  },
  {
   "source": [
    "##Reloading the Gensim and other Classification Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfname = \"gensim_5mc_NuAcid_embeddings.gen\"\n",
    "NuAcid_embed_model = gensim.models.Word2Vec.load(wfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "W0314 22:31:18.380050 10360 deprecation_wrapper.py:119] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0314 22:31:18.389051 10360 deprecation_wrapper.py:119] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0314 22:31:18.467048 10360 deprecation_wrapper.py:119] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0314 22:31:18.471052 10360 deprecation_wrapper.py:119] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0314 22:31:18.482055 10360 deprecation.py:506] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0314 22:31:18.682049 10360 deprecation_wrapper.py:119] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0314 22:31:21.116055 10360 deprecation_wrapper.py:119] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0314 22:31:21.131056 10360 deprecation.py:323] From C:\\Miniconda3\\envs\\py36tfkeras\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model_store_name = \"5mc-2models-dict21Mar05.jbl\"\n",
    "with open (model_store_name, \"rb\") as fp_handle:\n",
    "    model_dict = joblib.load(fp_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 20)          80        \n_________________________________________________________________\ncu_dnnlstm_1 (CuDNNLSTM)     (None, None, 30)          6240      \n_________________________________________________________________\ncu_dnngru_1 (CuDNNGRU)       (None, 20)                3120      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 20)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                210       \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 9,661\nTrainable params: 9,581\nNon-trainable params: 80\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(NA_embeddings)\n",
    "#model.add(Dropout(dropout_rate))\n",
    "#model_bd_gru.add(Bidirectional(GRU(rnn_hidden_size)))\n",
    "model_lstm.add(CuDNNLSTM(30, return_sequences=True))\n",
    "model_lstm.add(CuDNNGRU(20, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.5))\n",
    "model_lstm.add(Dense(10, activation='relu'))\n",
    "#model_gru.add(Dropout(0.1))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 53733 samples, validate on 13434 samples\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.2289 - acc: 0.9053 - val_loss: 0.2125 - val_acc: 0.9110\n",
      "Epoch 2/10\n",
      " - 5s - loss: 0.2241 - acc: 0.9066 - val_loss: 0.2206 - val_acc: 0.9123\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.2202 - acc: 0.9090 - val_loss: 0.2126 - val_acc: 0.9075\n",
      "Epoch 4/10\n",
      " - 5s - loss: 0.2228 - acc: 0.9065 - val_loss: 0.2383 - val_acc: 0.9128\n",
      "Epoch 5/10\n",
      " - 5s - loss: 0.2286 - acc: 0.9043 - val_loss: 0.2153 - val_acc: 0.9125\n",
      "Epoch 6/10\n",
      " - 5s - loss: 0.2212 - acc: 0.9087 - val_loss: 0.2092 - val_acc: 0.9112\n",
      "Epoch 7/10\n",
      " - 5s - loss: 0.2204 - acc: 0.9075 - val_loss: 0.2112 - val_acc: 0.9131\n",
      "Epoch 8/10\n",
      " - 5s - loss: 0.2232 - acc: 0.9069 - val_loss: 0.2141 - val_acc: 0.9088\n",
      "Epoch 9/10\n",
      " - 5s - loss: 0.2209 - acc: 0.9084 - val_loss: 0.2166 - val_acc: 0.9122\n",
      "Epoch 10/10\n",
      " - 5s - loss: 0.2208 - acc: 0.9076 - val_loss: 0.2128 - val_acc: 0.9135\n",
      "Model training took 50.09367656707764 Secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "chkptr_lstm = ModelCheckpoint(filepath=\"5mcLSTM.h5\", verbose=0, \\\n",
    "                               save_weights_only=False, save_best_only=True, period =5)\n",
    "tic = time()\n",
    "hist_srnn= model_lstm.fit(X_train, y_train, verbose=2,\n",
    "          batch_size= 128,\n",
    "          callbacks=[chkptr_lstm,earlystop,TonNaN, rLR],\n",
    "          epochs=10,validation_split= 0.2)\n",
    "#validation_data=(X_test, y_test))\n",
    "toc = time()\n",
    "print(\"Model training took {} Secs\".format ((toc-tic)))\n",
    "model_dict[\"LSTM\"] = model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_dict = dict()\n",
    "ypredprob_dict = dict()\n",
    "ypred_dict[\"CNN\"] = model_dict[\"CNN\"].predict(X_test)\n",
    "ypredprob_dict[\"CNN\"] = model_dict[\"CNN\"].predict(X_test)\n",
    "\n",
    "ypred_dict[\"GRU\"] = model_dict[\"GRU\"].predict(X_test)\n",
    "ypredprob_dict[\"GRU\"] = model_dict[\"GRU\"].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[12161  2248]\n [  308 14069]]\n              precision    recall  f1-score   support\n\n           0       0.98      0.84      0.90     14409\n           1       0.86      0.98      0.92     14377\n\n    accuracy                           0.91     28786\n   macro avg       0.92      0.91      0.91     28786\nweighted avg       0.92      0.91      0.91     28786\n\n0.9112068366567081\n0.8300120776794688\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, matthews_corrcoef\n",
    "y_pred = model_lstm.predict(X_test)\n",
    "y_pred = np.where (y_pred < 0.5, 0, 1)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.matthews_corrcoef(y_test,y_pred))\n",
    "ypred_dict[\"LSTM\"] = y_pred\n",
    "ypredprob_dict[\"LSTM\"] = model_lstm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"LSTM\"] = model_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 20)          80        \n_________________________________________________________________\nbidirectional_5 (Bidirection (None, None, 46)          6210      \n_________________________________________________________________\nbidirectional_6 (Bidirection (None, 24)                4320      \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 24)                0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 10)                250       \n_________________________________________________________________\ndense_8 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 10,871\nTrainable params: 10,791\nNon-trainable params: 80\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model_bd_gru = Sequential()\n",
    "model_bd_gru.add(NA_embeddings)\n",
    "#model.add(Dropout(dropout_rate))\n",
    "#model_bd_gru.add(Bidirectional(GRU(rnn_hidden_size)))\n",
    "model_bd_gru.add(Bidirectional(CuDNNGRU(23, return_sequences=True)))\n",
    "model_bd_gru.add(Bidirectional(CuDNNGRU(12, return_sequences=False)))\n",
    "model_bd_gru.add(Dropout(0.5))\n",
    "model_bd_gru.add(Dense(10, activation='relu'))\n",
    "model_bd_gru.add(Dense(1, activation='sigmoid'))\n",
    "model_bd_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_bd_gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 53733 samples, validate on 13434 samples\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.1968 - acc: 0.9240 - val_loss: 0.1738 - val_acc: 0.9296\n",
      "Epoch 2/10\n",
      " - 8s - loss: 0.1946 - acc: 0.9247 - val_loss: 0.1712 - val_acc: 0.9310\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.1915 - acc: 0.9259 - val_loss: 0.1718 - val_acc: 0.9316\n",
      "Epoch 4/10\n",
      " - 8s - loss: 0.1888 - acc: 0.9256 - val_loss: 0.1696 - val_acc: 0.9323\n",
      "Epoch 5/10\n",
      " - 8s - loss: 0.1883 - acc: 0.9267 - val_loss: 0.1717 - val_acc: 0.9317\n",
      "Epoch 6/10\n",
      " - 8s - loss: 0.2109 - acc: 0.9163 - val_loss: 0.1727 - val_acc: 0.9297\n",
      "Epoch 7/10\n",
      " - 8s - loss: 0.1873 - acc: 0.9273 - val_loss: 0.1694 - val_acc: 0.9309\n",
      "Epoch 8/10\n",
      " - 8s - loss: 0.1830 - acc: 0.9291 - val_loss: 0.1704 - val_acc: 0.9318\n",
      "Epoch 9/10\n",
      " - 8s - loss: 0.1831 - acc: 0.9283 - val_loss: 0.1821 - val_acc: 0.9257\n",
      "Epoch 10/10\n",
      " - 8s - loss: 0.1813 - acc: 0.9289 - val_loss: 0.1663 - val_acc: 0.9335\n",
      "Model training took 76.03562021255493 Secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "chkptr_bdgru = ModelCheckpoint(filepath=\"5mcBDGru.h5\", verbose=0, \\\n",
    "                               save_weights_only=False, save_best_only=True, period =5)\n",
    "tic = time()\n",
    "hist_bdgr= model_bd_gru.fit(X_train, y_train, verbose=2,\n",
    "          batch_size= 128,\n",
    "          callbacks=[chkptr_bdgru,earlystop,TonNaN, rLR],\n",
    "          epochs=10,validation_split= 0.2)\n",
    "#validation_data=(X_test, y_test))\n",
    "toc = time()\n",
    "print(\"Model training took {} Secs\".format ((toc-tic)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[12932  1477]\n [  513 13864]]\n              precision    recall  f1-score   support\n\n           0       0.96      0.90      0.93     14409\n           1       0.90      0.96      0.93     14377\n\n    accuracy                           0.93     28786\n   macro avg       0.93      0.93      0.93     28786\nweighted avg       0.93      0.93      0.93     28786\n\n0.9308691725144167\n0.8636875770657437\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_bd_gru.predict(X_test)\n",
    "y_pred = np.where (y_pred < 0.5, 0, 1)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.matthews_corrcoef(y_test,y_pred))\n",
    "ypred_dict[\"BiDirectionGRU\"] = y_pred\n",
    "ypredprob_dict[\"BiDirectionGRU\"] = model_bd_gru.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"BiDirection_GRU\"] = model_bd_gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 20)          80        \n_________________________________________________________________\nbidirectional_7 (Bidirection (None, None, 46)          8280      \n_________________________________________________________________\nbidirectional_8 (Bidirection (None, 24)                5760      \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 24)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 10)                250       \n_________________________________________________________________\ndense_10 (Dense)             (None, 1)                 11        \n=================================================================\nTotal params: 14,381\nTrainable params: 14,301\nNon-trainable params: 80\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model_bd_lstm = Sequential()\n",
    "model_bd_lstm.add(NA_embeddings)\n",
    "#model.add(Dropout(dropout_rate))\n",
    "#model_bd_gru.add(Bidirectional(GRU(rnn_hidden_size)))\n",
    "model_bd_lstm.add(Bidirectional(CuDNNLSTM(23, return_sequences=True)))\n",
    "model_bd_lstm.add(Bidirectional(CuDNNLSTM(12, return_sequences=False)))\n",
    "model_bd_lstm.add(Dropout(0.5))\n",
    "model_bd_lstm.add(Dense(10, activation='relu'))\n",
    "model_bd_lstm.add(Dense(1, activation='sigmoid'))\n",
    "model_bd_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_bd_lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 53733 samples, validate on 13434 samples\n",
      "Epoch 1/20\n",
      " - 8s - loss: 0.1728 - acc: 0.9281 - val_loss: 0.1677 - val_acc: 0.9317\n",
      "Epoch 2/20\n",
      " - 8s - loss: 0.1760 - acc: 0.9262 - val_loss: 0.2176 - val_acc: 0.9104\n",
      "Epoch 3/20\n",
      " - 8s - loss: 0.1733 - acc: 0.9278 - val_loss: 0.1657 - val_acc: 0.9336\n",
      "Epoch 4/20\n",
      " - 8s - loss: 0.1714 - acc: 0.9291 - val_loss: 0.1652 - val_acc: 0.9330\n",
      "Epoch 5/20\n",
      " - 8s - loss: 0.1752 - acc: 0.9277 - val_loss: 0.1659 - val_acc: 0.9332\n",
      "Epoch 6/20\n",
      " - 8s - loss: 0.1809 - acc: 0.9254 - val_loss: 0.1678 - val_acc: 0.9320\n",
      "Epoch 7/20\n",
      " - 8s - loss: 0.1696 - acc: 0.9307 - val_loss: 0.1657 - val_acc: 0.9331\n",
      "Epoch 8/20\n",
      " - 8s - loss: 0.1681 - acc: 0.9313 - val_loss: 0.1633 - val_acc: 0.9338\n",
      "Epoch 9/20\n",
      " - 8s - loss: 0.1685 - acc: 0.9306 - val_loss: 0.1640 - val_acc: 0.9335\n",
      "Epoch 10/20\n",
      " - 8s - loss: 0.1865 - acc: 0.9238 - val_loss: 0.1674 - val_acc: 0.9323\n",
      "Epoch 11/20\n",
      " - 8s - loss: 0.1704 - acc: 0.9311 - val_loss: 0.1626 - val_acc: 0.9343\n",
      "Epoch 12/20\n",
      " - 8s - loss: 0.1669 - acc: 0.9318 - val_loss: 0.1660 - val_acc: 0.9314\n",
      "Epoch 13/20\n",
      " - 8s - loss: 0.1672 - acc: 0.9320 - val_loss: 0.1627 - val_acc: 0.9352\n",
      "Epoch 14/20\n",
      " - 8s - loss: 0.1667 - acc: 0.9327 - val_loss: 0.1617 - val_acc: 0.9353\n",
      "Epoch 15/20\n",
      " - 8s - loss: 0.1656 - acc: 0.9332 - val_loss: 0.1612 - val_acc: 0.9340\n",
      "Epoch 16/20\n",
      " - 8s - loss: 0.1693 - acc: 0.9312 - val_loss: 0.1840 - val_acc: 0.9243\n",
      "Epoch 17/20\n",
      " - 8s - loss: 0.1655 - acc: 0.9329 - val_loss: 0.1637 - val_acc: 0.9339\n",
      "Epoch 18/20\n",
      " - 8s - loss: 0.1647 - acc: 0.9324 - val_loss: 0.1637 - val_acc: 0.9335\n",
      "Epoch 19/20\n",
      " - 8s - loss: 0.1677 - acc: 0.9324 - val_loss: 0.1616 - val_acc: 0.9347\n",
      "Epoch 20/20\n",
      " - 8s - loss: 0.1732 - acc: 0.9308 - val_loss: 0.1639 - val_acc: 0.9335\n",
      "Model training took 163.00331616401672 Secs\n"
     ]
    }
   ],
   "source": [
    "chkptr_bdlstm = ModelCheckpoint(filepath=\"5mcBDLSTM.h5\", verbose=0, \\\n",
    "                               save_weights_only=False, save_best_only=True, period =5)\n",
    "tic = time()\n",
    "hist_bdls= model_bd_lstm.fit(X_train, y_train, verbose=2,\n",
    "          batch_size= 128,\n",
    "          callbacks=[chkptr_bdlstm, earlystop, TonNaN, rLR],\n",
    "          epochs=20,validation_split= 0.2)\n",
    "#validation_data=(X_test, y_test))\n",
    "toc = time()\n",
    "print(\"Model training took {} Secs\".format ((toc-tic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[12897  1512]\n [  429 13948]]\n              precision    recall  f1-score   support\n\n           0       0.97      0.90      0.93     14409\n           1       0.90      0.97      0.93     14377\n\n    accuracy                           0.93     28786\n   macro avg       0.94      0.93      0.93     28786\nweighted avg       0.94      0.93      0.93     28786\n\n0.9325713888695893\n0.8676130953612733\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_bd_lstm.predict(X_test)\n",
    "y_pred = np.where (y_pred < 0.5, 0, 1)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(metrics.accuracy_score(y_test,y_pred))\n",
    "print(metrics.matthews_corrcoef(y_test,y_pred))\n",
    "ypred_dict[\"BiDirectionLSTM\"] = y_pred\n",
    "ypredprob_dict[\"BiDirectionLSTM\"] = model_bd_lstm.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict[\"BiDirection_LSTM\"] = model_bd_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_dict[\"Ground_truth\"] = y_test\n",
    "ypredprob_dict[\"Ground_truth\"] = y_test\n",
    "\n",
    "model_dict_path = Path(\"5cm-model-dict21Mar14.jbl\")\n",
    "model_dict_path.touch(exist_ok=True)\n",
    "\n",
    "with open(model_dict_path, \"wb\") as file_handle:\n",
    "    joblib.dump(model_dict, file_handle)\n",
    "\n",
    "pred_dict_path = Path(\"5cm-ypred-dict21Mar14.jbl\")\n",
    "pred_dict_path.touch(exist_ok=True)\n",
    "\n",
    "with open(pred_dict_path, \"wb\") as file_handle:\n",
    "    joblib.dump(ypred_dict,file_handle)\n",
    "\n",
    "x_test_path = Path(\"5cm-xtest-21Mar14.jbl\")\n",
    "x_test_path.touch(exist_ok=True)\n",
    "with open(x_test_path, \"wb\") as file_handle:\n",
    "    joblib.dump(X_test,file_handle)\n",
    "\n",
    "pred_dict_path = Path(\"5cm-ypredprob-dict21Mar14.jbl\")\n",
    "pred_dict_path.touch(exist_ok=True)\n",
    "with open(pred_dict_path, \"wb\") as file_handle:\n",
    "    joblib.dump(ypredprob_dict,file_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}